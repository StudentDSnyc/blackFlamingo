# Load Libraries
require(ggplot2)
require(caret)
require(gam)
require(dplyr)

# Load Helper Functions
source("Helpers.R")

##################
# Load
##################
load("./data/houses.train.RData")
load("./data/houses.test.RData")

#########################
# Create private test set
#########################

# Split our data into a train and test set - 80/20
set.seed(0)
split.ratio = 0.8
train.indices = sample(1:nrow(houses.train), nrow(houses.train)*split.ratio)
private.train = houses.train[train.indices,] # dim: 1168, 80 + engineered
private.test = houses.train[-train.indices,] # dim: 292, 80 + engineered

# Convert to data.frame (from data.table) 
private.train <- as.data.frame(private.train)
private.test <- as.data.frame(private.test)
houses.train <- as.data.frame(houses.train)
houses.test <- as.data.frame(houses.test)

##################
# One hot Encoding
##################

# One-hot encode categorical features using vtreat
# Scale all features including dummy ones per: https://stats.stackexchange.com/questions/69568/whether-to-rescale-indicator-binary-dummy-predictors-for-lasso

# Using vtreat (with data.frame)
##############

# Create encoder 
encoder <- vtreat::designTreatmentsN(dframe = private.train, # theoretically should use separate data to encode
                                     varlist = colnames(private.train),
                                     outcomename = "SalePrice",
                                     rareCount=10,
                                     rareSig=1,
                                     verbose=TRUE) 
# Now encode both train and test
encoded.private.train <- vtreat::prepare(encoder,
                                         private.train,
                                         pruneSig=1,
                                         scale = FALSE)

encoded.private.test <- vtreat::prepare(encoder,
                                        private.test,
                                        pruneSig=1,
                                        scale = FALSE)

# Same for houses.train/ houses.test
encoder <- vtreat::designTreatmentsN(dframe = houses.train, # theoretically should use separate data to encode
                                     varlist = colnames(houses.train),
                                     outcomename = "SalePrice",
                                     rareCount=10,
                                     rareSig=1,
                                     verbose=TRUE) 
# Now encode both train and test
encoded.houses.train <- vtreat::prepare(encoder,
                                        houses.train,
                                        pruneSig=1,
                                        scale = TRUE)

encoded.houses.test <- vtreat::prepare(encoder,
                                       houses.test,
                                       pruneSig=1,
                                       scale = TRUE)


# Cut any linear combinations or duplicate columns generated by dummify
lincomb <- findLinearCombos(encoded.private.train)
# Check columns that would be removed
colnames(encoded.private.train)[lincomb$remove]
encoded.private.train.decorr <- encoded.private.train[,-lincomb$remove]
encoded.private.test.decorr <- encoded.private.test[,-lincomb$remove]

# Same for full sets
lincomb <- findLinearCombos(encoded.houses.train)
# Check columns that would be removed
colnames(encoded.houses.train)[lincomb$remove]
encoded.houses.train.decorr <- encoded.houses.train[,-lincomb$remove]
encoded.houses.test.decorr <- encoded.houses.test[,-lincomb$remove]

#######################
# Splines Linear Model
#######################

# First run simple linear
set.seed(123)
require(splines)
train.control <- trainControl("cv", 10, savePred=T)
fit <- train(log(SalePrice) ~ ., 
             data=encoded.houses.train.decorr,
             method="glm",
             trControl=train.control)

# show predictions from CV
head(fit$pred)
# RMSE
sqrt(mean((fit$pred$pred - fit$pred$obs)^2)) # 0.1505522
cbind(fit$pred$obs, fit$pred$pred, log(encoded.houses.train.decorr$SalePrice)) %>% head

# Test Linear with caret pre-processing ####

# Run simple linear
fit.caret.preprocess <- train(log(SalePrice) ~ ., 
                              data=encoded.houses.train,
                              preProcess = "pca",
                              method="glm",
                              trControl=trainControl("cv", 10, savePred=T))

fit.caret.preprocess # RMSE 0.1445636
# Predict on same training data
pred.caret.preprocess  <- predict(fit.caret.preprocess, newdata = encoded.houses.train)
cbind(pred.caret.preprocess , log(encoded.houses.train.decorr$SalePrice)) %>% head
sqrt(mean((pred.caret.preprocess  - log(encoded.houses.train.decorr$SalePrice))^2)) 

encoded.houses.train.withpred <- cbind(encoded.houses.train, pred.caret.preprocess)
# Spline using initial linear prediction
# spline.fit <- glm(log(SalePrice) ~ .-pred.caret.preprocess +bs(pred.caret.preprocess, df=6), # df=6
#                   data=encoded.houses.train.withpred)

spline.fit <- train(log(SalePrice) ~ .-pred.caret.preprocess +bs(pred.caret.preprocess, df=10), 
                    data=encoded.houses.train.withpred,
                    preProcess = "pca",
                    method="glm",
                    trControl=trainControl("cv", 10, savePred=T))
# RMSE
sqrt(mean((spline.fit$pred$pred - spline.fit$pred$obs)^2)) # 0.143705 no meaninful improvement

# Running with the variable for doing the splines only
spline.test <- train(log(SalePrice) ~ bs(pred.caret.preprocess, df=7), 
                    data=encoded.houses.train.withpred,
                    # preProcess = "pca",
                    method="glm",
                    trControl=trainControl("cv", 10, savePred=T))
# RMSE
sqrt(mean((spline.test$pred$pred - spline.test$pred$obs)^2)) # 0.1323838 improvement

# Spline Cross Validation
set.seed(1331)
n_folds <- 5
folds <- createFolds(rownames(encoded.houses.train), k=n_folds)
splines.predictions <- list()

degrees_freedom <- 5
for (i in 1:n_folds) {
  spline.fit <- glm(log(SalePrice) ~ bs(pred.caret.preprocess, df=degrees_freedom), # df=6
                    data=encoded.houses.train.withpred[-folds[[i]], ])
  splines.predictions[[i]] <- predict(spline.fit, 
                                      newdata = encoded.houses.train.withpred[folds[[i]], -which(names(encoded.houses.train.withpred) == "SalePrice")])
  # RMSE for the fold
  RMSE <- sqrt(mean((splines.predictions[[i]] - log(encoded.houses.train.withpred[folds[[i]],c("SalePrice")]))^2))
  cat(RMSE)
  RMSE.folds[[i]] <- RMSE
}
# Mean RMSE
mean(as.numeric(RMSE.folds)) # 0.1316224 some improvement


# write.csv(data.frame(Id = 1:1459, splines_predictions = exp(splines.predictions.test)), 
#           paste(format(Sys.time(),'%Y-%m-%d %H-%M-%S'), "splines_predictions.csv"), 
#           row.names = FALSE)

#######################
# Smoothing Spline
#######################

plot(encoded.houses.train$GrLivArea, encoded.houses.train$SalePrice,cex=.5,col="darkgrey")
title (" Smoothing Spline ")
fit=smooth.spline(encoded.houses.train$GrLivArea, 
                  encoded.houses.train$SalePrice,cv=TRUE)
fit$df
lines(fit,col="red",lwd=2)


#######################
# GAM
#######################
require(gam)
gam.fit <- gam(log(SalePrice) ~ .-GrLivArea +s(GrLivArea, df=6),
               data=encoded.houses.train)

set.seed(123)
train.control <- trainControl("cv", 10, savePred=T)
fit <- train(log(SalePrice) ~ .-GrLivArea +s(GrLivArea, df=6) 
             -YearBuilt +bs(YearBuilt, df=6), 
             data=encoded.houses.train,
             method="glm",
             trControl=train.control)

# show predictions from CV
head(fit$pred)
# RMSE
sqrt(mean((fit$pred$pred - fit$pred$obs)^2)) # 0.1493699 Not good compared to splines


