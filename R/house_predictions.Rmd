---
title: "House_predictions"
output: html_document
---

## OUTLINE  # TO DO replace with Paul's 


2. Loading Data
3. Visualizations
4. Pre-Processing
5. Model Training and Parameter Tuning
6. Variable Importance and Feature selection
7. Summary

## Setup


```{r global_options, include=FALSE}
# set up global Knit options for all chunks
knitr::opts_chunk$set(fig.width=6, fig.height=4, fig.align = "center", include=TRUE, echo=TRUE, warning=FALSE, message=FALSE, cache=T)
```

```{r Load_Packages}
library(data.table)
library(dplyr)
library(ggplot2)
library(Amelia) # for missmap (NAs)

```

```{r Load_Helpers}
source("Helpers.R") # functions definitions
```

## 2. Data Loading & Preparation

### 2.1 Load data from csv

```{r Load Data}
# Assumes data directory 
fetch_data <- function(filename) {
  dt <- fread(paste0("./data/", filename),  # faster option than read.csv
              header = TRUE,
              sep = ",",
              stringsAsFactors = TRUE)  # load text variables as categorical
  dt
}
  
houses.train <- fetch_data("train.csv")
houses.test <- fetch_data("test.csv")
```

```{r Exploration Numerical}
dim(houses.train)
dim(houses.test)
```

```{r }
head(houses.train)
```

```{r Data structure}
#summary(houses)  # Use if small number of variables
str(houses.train)
```

```{r}
sapply(houses.train, class)
```

### 2.2 Getting factor levels from data description

```{r factor_levels}
## get levels of categorical features from data description
factorLevel <- list()
conn <- file("./data/data_description.txt", open="r")
f <-readLines(conn)
for (line in f){
  if(!grepl("^[[:blank:]]", line) & grepl(": ", line)) {
    col_name <<- trimws(gsub(":.*", "", line))
  } else {
    level <- trimws(gsub("\t.*", "", line))
    if (level != "") {
      factorLevel[[col_name]] <- c(factorLevel[[col_name]], level)
    }
  }
}
close(conn)

print(factorLevel[1:3])
```

### 2.3 Checking factor levels with data

```{r check_levels}
## check if levels in description cover unique data values
for (varname in names(factorLevel)) {
  levelDiff <- setdiff(unique(houses.train[[varname]]), 
                       factorLevel[[varname]])
  if(length(levelDiff)) {
    print(paste(varname, 
                paste(levelDiff, collapse = ", "), 
                sep = ": "))
  }
}
```


### 2.4 Fixing level names

```{r fix_levels}
## fix those levels that don't match with data
## ignore "NA" as they will be considered as missing when converting categorical to factors

unique(houses.train$MSZoning)
factorLevel$MSZoning
factorLevel$MSZoning[2] <- "C (all)"

unique(houses.train$Neighborhood)
factorLevel$Neighborhood
factorLevel$Neighborhood[13] <- "NAmes"

unique(houses.train$BldgType)
factorLevel$BldgType
factorLevel$BldgType[c(2,3,5)] <- c("2fmCon","Duplex","Twnhs")

unique(houses.train$Exterior2nd)
factorLevel$Exterior2nd
factorLevel$Exterior2nd[c(17,6,3)] <- c("Wd Shng","CmentBd","Brk Cmn")

## Get levels that only appear in the dataset
for (varname in names(factorLevel)) {
  factorLevel[[varname]] <- intersect(factorLevel[[varname]],
                                      unique(houses.train[[varname]]))
}

## Re-run the previous cell to double check
```

### 2.5 Converting column data types

```{r convert_types}
## convert column datatype to numeric / factor
## On training dataset
for (varname in names(houses.train)[-1]) {
  if (varname %in% names(factorLevel)) {
    houses.train[[varname]] <- factor(houses.train[[varname]], 
                                     levels = factorLevel[[varname]])
  } else {
    houses.train[[varname]] <- as.numeric(houses.train[[varname]])
  }
}

## On testing dataset
for (varname in names(houses.test)[-1]) {
  if (varname %in% names(factorLevel)) {
    houses.test[[varname]] <- factor(houses.test[[varname]], 
                                    levels = factorLevel[[varname]])
  } else {
    houses.test[[varname]] <- as.numeric(houses.test[[varname]])
  }
}
```


### 2.6 (Optional) Saving data

```{r save_loaded}
houses.train$Id <- NULL
rownames(houses.test) <- houses.test$Id
houses.test$Id <- NULL
save(houses.train, houses.test, file = "./house_loaded.RData")
```

## 3. Visualizations

### Loading data (from step 2)

```{r read_loaded}
library(ggplot2)
library(gridExtra)
library(tabplot)
library(lsr)
library(corrplot)
library(dplyr)

rm(list = ls())
load("./house_loaded.RData")
```


### 3.1 SalePrice Histogram

```{r hist_saleprice}
## histogram on SalePrice
grid.arrange(ggplot(houses.train) + 
               geom_histogram(aes(SalePrice), bins = 20), 
             ggplot(houses.train) + 
               geom_histogram(aes(log(SalePrice + 1)), bins = 20), 
             ncol = 2)
```

### 3.2 Plotting all features sorted by SalePrice

```{r table_plot, fig.height=4, fig.width=10}  
## table plot all features on sortded SalePrice
colMtx <- matrix(names(houses.train)[1:length(houses.train)-1], nrow = 8)
for (i in 1:ncol(colMtx)) {
  tableplot(houses.train, 
            select_string = c(colMtx[,i], "SalePrice"), 
            sortCol = "SalePrice", decreasing = TRUE, 
            nBins = 30)
}
```

Question: do we keep all this correlation stuff from Thomas?

### 3.3 Correlations between Continuous Variables

```{r corrplot_numerical}
numeric_features <- names(houses.train)[sapply(houses.train, is.numeric)]
numeric_features <- numeric_features[-length(numeric_features)]
print(numeric_features)
## correlation between continuous variables in training dataset - pearson
corr_numtran <- cor(houses.train %>% 
                      select(one_of(numeric_features, "SalePrice")), 
                    method = "pearson", 
                    use = "pairwise.complete.obs")

corrplot(corr_numtran, method = "color", order="hclust")

## correlation between continuous variables in test dataset - pearson
# corr_numtest <- cor(houses.test %>% 
#                       select(one_of(numeric_features)), 
#                     method = "pearson", 
#                     use = "pairwise.complete.obs")
# 
# corrplot(corr_numtest, method = "color", order="hclust")
```


### 3.4 Correlations between Ordinal Variables

```{r ordinal_features}
## ordinal features are those who contain one of the follow levels
ordinal_levels <- c("Reg", "5", "TA", "No", "Unf", 
                    "MnPrv", "Y", "Mod", "HLS", "1Fam")
ordinal_features <- character(0)

for(feature in names(houses.train)) {
  if(is.factor(houses.train[,feature]) && 
     length(intersect(ordinal_levels, levels(houses.train[,feature])))) {
    ordinal_features <- c(ordinal_features, feature)
  }
}

print(ordinal_features)
```

```{r corrplot_ordinal}
## correlation between ordinal variables in training dataset - kendall
corr_ordtran <- cor(data.matrix(houses.train %>% 
                                  select(one_of(ordinal_features))), 
                    method = "kendall", 
                    use = "pairwise.complete.obs")

corrplot(corr_ordtran, method = "color", order="hclust")

## correlation between ordinal variables in test dataset - kendall
# corr_ordtest <- cor(data.matrix(houses.test %>% 
#                                   select(one_of(ordinal_features))), 
#                     method = "kendall", 
#                     use = "pairwise.complete.obs")
# 
# corrplot(corr_ordtest, method = "color", order="hclust")
```

### 3.5 Correlations between Nominal Variables

```{r corrplot_nominal}
## Cram√©r's V is a measure of association between two nominal variables, giving a value between 0 and +1 (inclusive)
cor.cramersV <- function(data) {
  cramersV(table(data[complete.cases(data),]))
}

nominal_features <- setdiff(names(houses.train), 
                            c(numeric_features, ordinal_features, "SalePrice"))


## cramers V in test dataset
corr_nomtran <- sapply(nominal_features, 
                       function(x) sapply(nominal_features,
                                          function(y) cor.cramersV(houses.train[, c(x, y)])))

corrplot(corr_nomtran, method = "color", order="hclust")
```

### 3.6 Correlations between Ordinal Variables vs. SalePrice

```{r corrplot_ordinal_2}
## coorelation between ordered categorical variables in training - spearman
cor.ordcnt <- function(data, x, y) {
  cor(as.numeric(data[[x]]), as.numeric(data[[y]]), 
                 method = "spearman", 
                 use = "pairwise.complete.obs")
}

corr_ordcnttran <- data.frame(Variable = ordinal_features,
                              Correlation = sapply(ordinal_features, 
                                                function(x) -cor.ordcnt(houses.train, x, "SalePrice")))

ggplot(corr_ordcnttran, aes(reorder(Variable, Correlation), Correlation)) + 
  geom_bar(stat = "identity") +
  coord_flip()
```

```{r boxplot_ordinal}
## Might be a good idea to convert some ordinal predictors to continuous 
grid.arrange(
  ggplot(houses.train, aes(x = OverallQual, y = SalePrice)) + geom_boxplot(),
  ggplot(houses.train, aes(x = ExterQual, y = SalePrice)) + geom_boxplot(),
  ggplot(houses.train, aes(x = BsmtQual, y = SalePrice)) + geom_boxplot(),
  ggplot(houses.train, aes(x = KitchenQual, y = SalePrice)) + geom_boxplot(),
  ggplot(houses.train, aes(x = GarageFinish, y = SalePrice)) + geom_boxplot(),
  ggplot(houses.train, aes(x = FireplaceQu, y = SalePrice)) + geom_boxplot(),
  ncol = 2
)
grid.arrange(
  ggplot(houses.train, aes(x = as.integer(OverallQual), y = SalePrice)) + geom_point(),
  ggplot(houses.train, aes(x = as.integer(ExterQual), y = SalePrice)) + geom_point(),
  ggplot(houses.train, aes(x = as.integer(BsmtQual), y = SalePrice)) + geom_point(),
  ggplot(houses.train, aes(x = as.integer(KitchenQual), y = SalePrice)) + geom_point(),
  ggplot(houses.train, aes(x = as.integer(GarageFinish), y = SalePrice)) + geom_point(),
  ggplot(houses.train, aes(x = as.integer(FireplaceQu), y = SalePrice)) + geom_point(),
  ncol = 2
)

tableplot(houses.train %>% select(one_of("SalePrice","OverallQual", "ExterQual", "BsmtQual",
                                        "KitchenQual", "GarageFinish", "FireplaceQu")), 
          decreasing = TRUE, 
          nBins = 18,
          colorNA = "#FF1414", colorNA_num = "#FF1414")
```


## 4. Pre-Processing

### 4.1 Missingness

```{r Missingness_Map}
houses.test$SalePrice <- 0 # so train and test have same dimension
missmap(rbind(houses.train, houses.test), legend = TRUE, col = c("yellow","black"), main = "House Price Missing Data Map")
# missing values: PoolQC, MiscFeature, Alley, Fence, FirePlaceQu, LotFrontage...
```

```{r NA_Analysis}
NA.analysis(rbind(houses.train, houses.test))

```

### Releveling variables for which 'NA' does not represent a missing value but rather it represents 'Not Applicable' according to the variable documentation

```{r }
#Alley
house$Alley = as.character(house$Alley)
house$Alley[is.na(house$Alley)] = 'NA'
house$Alley = as.factor(house$Alley)
#levels(house$Alley) #Uncomment to inspect

#Fence
house$Fence = as.character(house$Fence)
house$Fence[is.na(house$Fence)] = 'NA'
house$Fence = as.factor(house$Fence)
#levels(house$Fence) 

#GarageFinish
house$GarageFinish = as.character(house$GarageFinish)
house$GarageFinish[is.na(house$GarageFinish)] = 'NA'
house$GarageFinish = as.factor(house$GarageFinish)
#levels(house$GarageFinish) 

#GarageYrBlt
house$GarageYrBlt = as.character(house$GarageYrBlt)
house$GarageYrBlt[is.na(house$GarageYrBlt)] = 'NA'
house$GarageYrBlt = as.factor(house$GarageYrBlt)
#levels(house$GarageYrBlt) 
# There is a problem with the second to last level here: we have a year of 2207. Will replace
# with 2007:
levels(house$GarageYrBlt)[103] = "2007"
#levels(house$GarageYrBlt) 

#GarageType
house$GarageType = as.character(house$GarageType)
house$GarageType[is.na(house$GarageType)] = 'NA'
house$GarageType = as.factor(house$GarageType)
#levels(house$GarageType) 

#BsmtExposure
house$BsmtExposure = as.character(house$BsmtExposure)
house$BsmtExposure[is.na(house$BsmtExposure)] = 'NA'
house$BsmtExposure = as.factor(house$BsmtExposure)
#levels(house$BsmtExposure)

#BsmtCond
house$BsmtCond = as.character(house$BsmtCond)
house$BsmtCond[is.na(house$BsmtCond)] = 'NA'
house$BsmtCond = as.factor(house$BsmtCond)
#levels(house$BsmtCond)

#BsmtQual
house$BsmtQual = as.character(house$BsmtQual)
house$BsmtQual[is.na(house$BsmtQual)] = 'NA'
house$BsmtQual = as.factor(house$BsmtQual)
#levels(house$BsmtQual)

#BsmtFinType1
house$BsmtFinType1 = as.character(house$BsmtFinType1)
house$BsmtFinType1[is.na(house$BsmtFinType1)] = 'NA'
house$BsmtFinType1 = as.factor(house$BsmtFinType1)
#levels(house$BsmtFinType1)

#BsmtFinType2
house$BsmtFinType2 = as.character(house$BsmtFinType2)
house$BsmtFinType2[is.na(house$BsmtFinType2)] = 'NA'
house$BsmtFinType2 = as.factor(house$BsmtFinType2)
#levels(house$BsmtFinType2)

#FireplaceQu
house$FireplaceQu = as.character(house$FireplaceQu)
house$FireplaceQu[is.na(house$FireplaceQu)] = 'NA'
house$FireplaceQu = as.factor(house$FireplaceQu)
#levels(house$FireplaceQu)

#MiscFeature
house$MiscFeature = as.character(house$MiscFeature)
house$MiscFeature[is.na(house$MiscFeature)] = 'NA'
house$MiscFeature = as.factor(house$MiscFeature)
#levels(house$MiscFeature)

#PoolQC
house$PoolQC = as.character(house$PoolQC)
house$PoolQC[is.na(house$PoolQC)] = 'NA'
house$PoolQC = as.factor(house$PoolQC)
#levels(house$PoolQC)


```

### **TO DISCUSS**: Missingness in LotFrontage appears to be related to the LotConfig variable in that over half of the missing values come from LotConfigs of 'Inside'. Consider imputing with the mean LotFrontage value for LotConfigs == 'Inside'.

```{r}
library(dplyr)
lot.info = house %>% select(LotFrontage, LotConfig, LotArea, LotShape)
lf.missing = lot.info[is.na(lot.info$LotFrontage),]
group_by(lf.missing, LotConfig) %>% summarise(percent = n()/nrow(lf.missing))
```

```{r}

```

```{r}

```



### 4. Transforming `SalePrice` to log scale

```{r logtrans}
## Transform SalePrice to log scale
houses.train$SalePrice <- log(houses.train$SalePrice + 1)
```

### 4.8 (Optional) Saving data

```{r save_preProc}
save(houses.train, houses.test, file = "./house_preProc.RData")
```

# Modeling

## 5.1 Model Training and Parameter Tuning

### Loading data (from step 4)

```{r read_preProc}
rm(list = ls()) # clear workspace
library(caret)
load("./house_preProc.RData")
```

### 5.2 Data Splitting using `createDataPartiton`

```{r split_data}
## Perform single 80%/20% random split of houses.train
library(caret)
set.seed(321)
trainIdx <- createDataPartition(houses.train$SalePrice, 
                                p = .8,
                                list = FALSE,
                                times = 1)
subTrain <- houses.train[trainIdx,]
subTest <- houses.train[-trainIdx,]
print(head(subTrain))
```

### 5.2 Setting up resampling method using `trainControl`

```{r trainCtrl}
set.seed(456)
fitCtrl <- trainControl(method = "repeatedcv",
                        number = 5,
                        repeats = 3,
                        verboseIter = FALSE,
                        summaryFunction = defaultSummary)
```


## 6.1 Basic Linear Regression
First of all let's build a <strong>multiple linear model</strong> to use all predictors to predict <strong>SalePrice</strong>:

- Train ML model with <strong>train</strong>
- Evaluate variable importance
- Predict on test dataset with <strong>predict</strong>
- Measure performance

```{r linearreg}
lmFit <- train(SalePrice ~., data = subTrain,
               method = "lm")
# summary(lmFit)
## Call:
## lm(formula = .outcome ~ ., data = dat)
## ... ...
## Residual standard error: 0.1152 on 915 degrees of freedom
## Multiple R-squared:  0.935,	Adjusted R-squared:  0.917 
## F-statistic:    52 on 253 and 915 DF,  p-value: < 2.2e-16
```

### 6.2 Linear Regression Variable Importance

```{r linreg_var_imp}
lmImp <- varImp(lmFit, scale = FALSE)
## lm variable importance
##
##  only 20 most important variables shown (out of 253)
## 
##                      Overall
## MSZoningRL             8.151
## MSZoningFV             7.881
## MSZoningRM             7.659
## MSZoningRH             6.977
## SaleConditionAbnorml   5.433
## OverallQual1           5.309
## LandContourBnk         4.292
```

Note: linear models use the absolute value of the t-statistic.

```{r linreg_var_importance}
plot(lmImp, top = 20)
```

### 6.3 Performance Measures for Linear Regression

```{r linreg_train_rmse}
mean(lmFit$resample$RMSE)
```
```{r linreg_test_rmse}
predicted <- predict(lmFit, subTest)
RMSE(pred = predicted, obs = subTest$SalePrice)
```
```{r linreg_predictions_plot}
ggplot(subTest, aes(x = exp(SalePrice)-1, y = exp(predicted)-1)) +
  geom_point() + 
  coord_fixed()
```



## 7.1 Linear Regression with Elastic Net Regularization - Grid search with `train`

```{r train_lasso}
enetGrid <- expand.grid(alpha = seq(0, 1, .1),
                        lambda = seq(0, .6, .01))

set.seed(1234)  # for reproducibility
enetFit <- train(SalePrice ~ .,
                 data = subTrain,
                 method="glmnet",
                 metric="RMSE",
                 trControl=fitCtrl,
                 tuneGrid=enetGrid)
print(enetFit$bestTune)
```

### 7.2 Choosing the Parameters
```{r reg_plots}
plot(enetFit)
plot(enetFit, plotType = "level")
```

### 7.3 Regularization Feature Importance
```{r reg_feature_imp}
enetVarImp <- varImp(enetFit, scale = FALSE)
plot(enetVarImp, top = 20)
mean(enetFit$resample$RMSE)
```

### 7.4 Analyzing the Regularized Regression Performance

```{r enet_mean_rmse}
mean(enetFit$resample$RMSE)
```
```{r enet_predicted_rmse}
predicted <- predict(enetFit, subTest)
RMSE(pred = predicted, obs = subTest$SalePrice)
```

```{r predict_lasso}

subTest$predicted <- predict(enetFit, subTest)
ggplot(subTest, aes(x = SalePrice, y = predicted)) + geom_point()
```


## 8.1 Tree-based Ensemble Models: Gradient Boosting Machines

```{r train_gbm, results="hide"}
fitCtrl <- trainControl(method = "cv",
                        number = 5,
                        verboseIter = TRUE,
                        summaryFunction=defaultSummary)
gbmGrid <- expand.grid( n.trees = seq(100,1000,100), 
                        interaction.depth = seq(1,10,2), 
                        shrinkage = c(0.1),
                        n.minobsinnode = 10)
gbmFit <- train(SalePrice ~ .,
                data = subTrain, 
                method = "gbm", 
                trControl = fitCtrl,
                tuneGrid=gbmGrid,
                metric='RMSE',
                maximize=FALSE)
```

### 8.2 gbm Parameters
```{r gmb_plot}
plot(gbmFit)
gbmFit$bestTune
```


### 8.3 Performance Measures for gbm

```{r g_train_rmse}
mean(gbmFit$resample$RMSE)
```
```{r regularization_test_rmse}
predicted <- predict(gbmFit, subTest)
RMSE(pred = predicted, obs = subTest$SalePrice)
```
```{r gbm_predictions_plot}
ggplot(subTest, aes(x = exp(SalePrice)-1, y = exp(predicted)-1)) +
  geom_point() +
  coord_fixed()
```


## 9. Summary

| Model                                         | RMSE   |
|-----------------------------------------------|--------|
| Linear Regression (all variables)             | 0.1630 |  
| Linear Regression with Regularization (Lasso) | 0.1368 |
| Gradient Boosting Machines                    | 0.1280 |

```{r}

```




```{r}

```

```{r}

```

```{r}

```

