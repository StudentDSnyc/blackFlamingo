{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stacked Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure for a 5 fold stacking may be described as follows:\n",
    "\n",
    "1. Split the total training set into two disjoint sets (here train and holdout)\n",
    "\n",
    "2. Train several base models on the first part (train)\n",
    "\n",
    "3. Predict these base models on the second part (holdout)\n",
    "\n",
    "4. Repeat step 1-3 five times and use the holdout predictions as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n",
    "\n",
    "\n",
    "- For the test set, we could either average the predictions of all base models on the test data or refit the model using the whole training set and then predict. Generally speaking, either way is fine because the test set hasn't seen the training set.\n",
    "- If we ran 10 models using the same procedure, our meta model will have 10 input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://s3.amazonaws.com/nycdsabt01/stacking.jpg)\n",
    "\n",
    "Borrowed from [Faron](https://www.kaggle.com/getting-started/18153#post103381)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick note, one should try a few diverse models. To my experience, a good stacking solution is often composed of at least:\n",
    "- 2 or 3 GBMs/XGBs/LightGBMs (one with low depth, one with medium and one with high)\n",
    "- 1 or 2 Random Forests (again as diverse as possible–one low depth, one high)\n",
    "- 1 linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, LinearRegression as lr\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr, RandomForestRegressor as rfr\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful if you are debugging the function inside another .py script\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "houses_train = pd.read_csv('../Data/encoded_houses_train.csv')\n",
    "houses_test = pd.read_csv('../Data/encoded_houses_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: SalePrice, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_test.SalePrice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = houses_train.loc[:, houses_train.columns != \"SalePrice\"].values # convert to np.array\n",
    "y_train = houses_train.loc[:, houses_train.columns == \"SalePrice\"].values.reshape(-1, )\n",
    "\n",
    "X_test = houses_test.loc[:, houses_train.columns != \"SalePrice\"].values # convert to np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stacking import stacking_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log(y), np.log(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # linear model, ElasticNet = lasso + ridge\n",
    "    ElasticNet(random_state=0, \n",
    "              fit_intercept=True, alpha=0.18069, l1_ratio=0.01),\n",
    "    \n",
    "    # deep random forst model\n",
    "    rfr(random_state=0,\n",
    "        n_estimators=1000, max_depth=20,  max_features=70),\n",
    "    \n",
    "    # aggressive random forst model\n",
    "    rfr(random_state=0, \n",
    "        n_estimators=1500, max_depth=10,  max_features=75),\n",
    "    \n",
    "    # conservative gbm model\n",
    "    gbr(random_state=0, learning_rate = 0.005, max_features='sqrt',\n",
    "        min_samples_leaf=15, min_samples_split=10, \n",
    "        n_estimators=3000, max_depth=3),\n",
    "    \n",
    "    # aggressive gbm model\n",
    "    gbr(random_state = 0, learning_rate = 0.01, max_features='sqrt',\n",
    "        min_samples_leaf=10, min_samples_split=5, \n",
    "        n_estimators = 1000, max_depth = 9),\n",
    "    \n",
    "    XGBRegressor(max_depth=3, \n",
    "                 learning_rate=0.03, \n",
    "                 n_estimators=1700, # Number of boosted trees to fit\n",
    "                 silent=True, # print messages while running \n",
    "                 objective='reg:linear', \n",
    "                 booster='gbtree', # Specify which booster to use: gbtree, gblinear or dart\n",
    "                 n_jobs=-1, # Number of parallel threads used to run xgboost. (replaces nthread)\n",
    "                 gamma=0,  # Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "                 min_child_weight=1, # Minimum sum of instance weight(hessian) needed in a child\n",
    "                 max_delta_step=0, # Maximum delta step we allow each tree’s weight estimation to be\n",
    "                 subsample=1, # Subsample ratio of the training instance\n",
    "                 colsample_bytree=1, # Subsample ratio of columns when constructing each tree\n",
    "                 colsample_bylevel=0.3, # Subsample ratio of columns for each split, in each level\n",
    "                 reg_alpha=0, # L1 regularization term on weights\n",
    "                 reg_lambda=1, # L2 regularization term on weights\n",
    "                 scale_pos_weight=1, # Balancing of positive and negative weights\n",
    "                 base_score=0.5, # The initial prediction score of all instances, global bias\n",
    "                 random_state=743, \n",
    "                 missing=None)       \n",
    "    ]\n",
    "\n",
    "meta_model = lr(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: [rmsle]\n",
      "\n",
      "model 0: [ElasticNet]\n",
      "    fold 0: [0.12733265]\n",
      "    fold 1: [0.15022274]\n",
      "    fold 2: [0.13589971]\n",
      "    ----\n",
      "    MEAN:   [0.13814301]\n",
      "\n",
      "model 1: [RandomForestRegressor]\n",
      "    fold 0: [0.13083948]\n",
      "    fold 1: [0.15361229]\n",
      "    fold 2: [0.12955067]\n",
      "    ----\n",
      "    MEAN:   [0.13844852]\n",
      "\n",
      "model 2: [RandomForestRegressor]\n",
      "    fold 0: [0.13149321]\n",
      "    fold 1: [0.15441378]\n",
      "    fold 2: [0.13017531]\n",
      "    ----\n",
      "    MEAN:   [0.13914579]\n",
      "\n",
      "model 3: [GradientBoostingRegressor]\n",
      "    fold 0: [0.11873860]\n",
      "    fold 1: [0.14330742]\n",
      "    fold 2: [0.11815123]\n",
      "    ----\n",
      "    MEAN:   [0.12727946]\n",
      "\n",
      "model 4: [GradientBoostingRegressor]\n",
      "    fold 0: [0.11989911]\n",
      "    fold 1: [0.14375247]\n",
      "    fold 2: [0.11856115]\n",
      "    ----\n",
      "    MEAN:   [0.12793496]\n",
      "\n",
      "model 5: [XGBRegressor]\n",
      "    fold 0: [0.11341232]\n",
      "    fold 1: [0.13488918]\n",
      "    fold 2: [0.11454766]\n",
      "    ----\n",
      "    MEAN:   [0.12135610]\n",
      "\n",
      "CPU times: user 1min 20s, sys: 546 ms, total: 1min 20s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stacking_prediction = stacking_regression(models, \n",
    "                                          rfr(random_state=0,\n",
    "                                              n_estimators=1000, \n",
    "                                              max_depth=20,  \n",
    "                                              max_features=2), \n",
    "                                          X_train, y_train, X_test,\n",
    "                                          transform_target=np.log1p, transform_pred = np.expm1, \n",
    "                                          metric=rmsle, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 127105.55992651,  136655.35847707,  135104.37020173,\n",
       "         133678.10663924,  132143.12258639,  146018.82291667],\n",
       "       [ 137412.03036976,  130008.91464806,  130594.41255991,\n",
       "         134093.65574022,  132753.1569182 ,  128873.3125    ],\n",
       "       [ 120600.16536177,  125999.55473139,  125837.58377347,\n",
       "         120684.7373917 ,  123356.66034635,  120459.921875  ],\n",
       "       [ 152045.71243471,  152824.85540069,  153856.58421047,\n",
       "         152363.4550493 ,  152477.38267359,  146918.74479167],\n",
       "       [ 141032.96167964,  149500.33968225,  149712.88825911,\n",
       "         148565.14198989,  151014.43535457,  144769.609375  ]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1459, 6)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1459,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_prediction[100:105]\n",
    "stacking_prediction.shape\n",
    "stacking_prediction.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214281.13531258886"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_prediction.mean(axis=1)[1458]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147500.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1460,)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1459]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 123284.27605339,  146191.75888207,  175551.22032681, ...,\n",
       "        146172.25115723,  121320.4997873 ,  194930.93656124])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 117727.38857337,  149803.51894442,  184443.48134752, ...,\n",
       "        155993.94550037,  108077.20099528,  215095.8125255 ])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Having more models than necessary in ensemble may hurt.**\n",
    "\n",
    "Lets say we have a library of created models. Usually greedy-forward approach works well:\n",
    "- Start with a few well-performing models’ ensemble\n",
    "- Loop through each other model in a library and add to current ensemble\n",
    "- Determine best performing ensemble configuration\n",
    "- Repeat until metric converged\n",
    "\n",
    "If you are using linear regression as the meta model, make sure you have **diverse/uncorrelated** first layer models\n",
    "\n",
    "During each loop iteration it is wise to consider only a subset of library models, which could work as a regularization for model selection.\n",
    "\n",
    "Repeating procedure few times and bagging results reduces the possibility of overfitting by doing model selection.\n",
    "\n",
    "Another [great walkthrough](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python) of stacking on Kaggle using the famous Titanic dataset.\n",
    "\n",
    "R users can call the `stackedEnsemble()` function from the [H2o package](https://h2o-release.s3.amazonaws.com/h2o/rel-ueno/2/docs-website/h2o-docs/data-science/stacked-ensembles.html) directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Success formula (personal opinion)\n",
    "\n",
    "50% - feature engineering\n",
    "\n",
    "30% - model diversity\n",
    "\n",
    "10% - luck\n",
    "\n",
    "10% - proper ensembling\n",
    " - Voting\n",
    " - Averaging\n",
    " - Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"Id\": range(1461, 2920), \n",
    "                            \"SalePrice\": stacking_prediction.mean(axis=1)})   # values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-11-11 18 59'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "time = '{:%Y-%m-%d %H %M}'.format(datetime.datetime.now())\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./Ensemble Submissions/submission {}.csv\".format(time), sep=',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
